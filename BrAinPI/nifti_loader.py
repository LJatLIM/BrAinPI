import zarr, os, itertools
import numpy as np
import hashlib
import shutil
import time
import nibabel as nib
from filelock import FileLock
from zarr.storage import NestedDirectoryStore
from collections.abc import MutableMapping
from zarr._storage.store import Store, BaseStore
from typing import Union
from niizarr import nii2zarr
# Path = Union[str, bytes, None]
from pathlib import Path
StoreLike = Union[BaseStore, Store, MutableMapping]
from logger_tools import logger
import gc
import multiprocessing
from utils import calculate_hash, get_directory_size, delete_oldest_files

def separate_process_generation(inp, out, time_axe):
    """
    Generate a Zarr dataset from a NIfTI file in a separate process.

    This function converts a NIfTI file into a Zarr dataset. The conversion process is handled
    using the `nii2zarr` function, which supports multi-resolution image storage.

    Args:
        inp (str): The file path to the input NIfTI file.
        out (str): The directory path where the output Zarr dataset will be stored.
        time_axe (bool): If True, the time axis is ignored during the conversion process.
    """
    nii2zarr(inp, out, no_time=time_axe)

# def calculate_hash(input_string):
#     # Calculate the SHA-256 hash of the input string
#     hash_result = hashlib.sha256(input_string.encode()).hexdigest()
#     return hash_result


# def get_directory_size(directory):
#     total_size = 0
#     for dirpath, dirnames, filenames in os.walk(directory):
#         for f in filenames:
#             fp = os.path.join(dirpath, f)
#             total_size += os.path.getsize(fp)
#     return total_size


# def delete_oldest_files(directory, size_limit):
#     items = sorted(Path(directory).glob("*"), key=os.path.getctime)
#     total_size = get_directory_size(directory)

#     # Delete oldest items until the total size is within the size limit
#     for item in items:
#         if total_size <= size_limit:
#             break
#         if item.is_file():
#             item_size = os.path.getsize(item)
#             os.remove(item)
#             total_size -= item_size
#             logger.success(f"Deleted file {item} of size {item_size} bytes")
#         elif item.is_dir():
#             dir_size = get_directory_size(item)
#             shutil.rmtree(item)
#             total_size -= dir_size
#             logger.success(f"Deleted directory {item} of size {dir_size} bytes")


class nifti_zarr_loader:
    """
    A loader class for handling NIfTI files with pyramid generation using Zarr format.
    """
    def __init__(
        self,
        location,
        pyramid_images_connection={},
        pyramids_images_allowed_store_size_gb=100,
        pyramids_images_allowed_generation_size_gb=2,
        pyramids_images_store=None,
        extension_type=".nii.zarr",
        ResolutionLevelLock=None,
        zarr_store_type: StoreLike = NestedDirectoryStore,
        verbose=None,
        squeeze=True,
        cache=None,
    ):
        """
        Initialize the nifti_zarr_loader object.

        Args:
            location (str): Path to the NIfTI file.
            pyramid_images_connection (dict): Mapping of hash values to pyramid images.
            pyramids_images_allowed_store_size_gb (float): Maximum allowed size for the pyramid images store in GB. Defaults to 100.
            pyramids_images_allowed_generation_size_gb (float): Maximum allowed size for pyramid generation in GB. Defaults to 2.
            pyramids_images_store (str, optional): Directory for storing pyramid images. Defaults to None.
            extension_type (str, optional): File extension for the generated pyramid images. Defaults to ".nii.zarr".
            ResolutionLevelLock (int, optional): Lock for accessing a specific resolution level. Defaults to None.
            zarr_store_type (zarr.storage, optional): Zarr store type. Defaults to NestedDirectoryStore.
            verbose (bool, optional): Verbose logging. Defaults to None.
            squeeze (bool, optional): Whether to remove singleton dimensions from arrays. Defaults to True.
            cache (object, optional): Cache object for storing slices. Defaults to None.
        """
        self.location = location
        self.datapath = location
        self.ResolutionLevelLock = (
            0 if ResolutionLevelLock is None else ResolutionLevelLock
        )
        self.file_stat = os.stat(location)
        self.filename = os.path.split(self.datapath)[1]
        self.file_ino = str(self.file_stat.st_ino)
        self.modification_time = str(self.file_stat.st_mtime)
        self.file_size = self.file_stat.st_size
        # self.allowed_store_size_gb = float(
        #     self.settings.get("nifti_loader", "pyramids_images_allowed_store_size_gb")
        # )
        self.allowed_store_size_gb = float(pyramids_images_allowed_store_size_gb)
        self.allowed_store_size_byte = self.allowed_store_size_gb * 1024 * 1024 * 1024
        # self.allowed_file_size_gb = float(
        #     self.settings.get("nifti_loader", "pyramids_images_allowed_generation_size_gb")
        # )
        self.allowed_file_size_gb = float(pyramids_images_allowed_generation_size_gb)
        self.allowed_file_size_byte = self.allowed_file_size_gb * 1024 * 1024 * 1024
        self.pyramid_dic = pyramid_images_connection
        self.pyramids_images_store = pyramids_images_store
        self.extension_type = extension_type
        self.verbose = verbose
        self.squeeze = squeeze
        self.cache = cache
        self.metaData = {}
        # Go through pyramid generation process for nii.gz files
        if self.datapath.endswith(".nii.gz") or self.datapath.endswith(".nii"):
            self.validate_nifti_file(self.datapath)
            self.pyramid_builders(self.datapath)
        # Open zarr store
        self.zarr_store = zarr_store_type  # Only relevant for non-s3 datasets
        store = self.zarr_store_type(
            self.datapath
        )
        zgroup = zarr.open(store)
        self.zattrs = zgroup.attrs

        if "omero" in self.zattrs:
            self.omero = zgroup.attrs["omero"]
        # assert 'omero' in self.zattrs
        # self.omero = zgroup.attrs['omero']
        assert "multiscales" in self.zattrs
        self.multiscales = zgroup.attrs["multiscales"]
        try:
            self.axes = self.multiscales[0]["axes"]
        except:
            self.axes = self.multiscales["axes"]
        # self.axes = self.multiscales[0]['axes']
        if len(self.axes) < 3:
            raise Exception()
        self.dim_pos_dic = {"t": None, "c": None, "z": None, "y": None, "x": None}
        self.space_unit = None
        for index, axe in enumerate(self.axes):
            self.dim_pos_dic[axe["name"]] = index
            if axe["type"] == "space":
                self.space_unit = axe["unit"]
        logger.info(self.dim_pos_dic)
        logger.info(self.multiscales)
        logger.info(self.space_unit)
        del zgroup
        del store

        try:
            self.multiscale_datasets = self.multiscales[0]["datasets"]
        except:
            self.multiscale_datasets = self.multiscales["datasets"]

        self.ResolutionLevels = len(self.multiscale_datasets)

        self.dataset_paths = []
        self.dataset_scales = []
        self.arrays = {}
        for r in range(self.ResolutionLevels):
            self.dataset_paths.append(self.multiscale_datasets[r]["path"])
            self.dataset_scales.append(
                self.multiscale_datasets[r]["coordinateTransformations"][0]["scale"]
            )
            array = self.open_array(r)
            if r == 0:
                self.TimePoints = (
                    array.shape[self.dim_pos_dic["t"]] if self.dim_pos_dic["t"] else 1
                )
                self.Channels = (
                    array.shape[self.dim_pos_dic["c"]] if self.dim_pos_dic["c"] else 1
                )
            # shape_z = array.shape[self.dim_pos_dic['z']]
            # shape_y = array.shape[self.dim_pos_dic['y']]
            # shape_x = array.shape[self.dim_pos_dic['x']]
            # if shape_z <= 64 and shape_y <= 64 and shape_x <=64:
            #     self.ResolutionLevels = r + 1
            #     break

            for t, c in itertools.product(range(self.TimePoints), range(self.Channels)):

                # Collect attribute info
                # self.metaData[r, t, c, "shape"] = array.shape
                # shape = array.shape
                # if len(shape) != 5:
                #     # Prepend 1 to the shape to make its length 5
                #     new_shape = (1,) * (5 - len(shape)) + shape
                #     self.metaData[r, t, c, "shape"] = new_shape
                # else:
                #     self.metaData[r, t, c, "shape"] = shape
                shape_z = array.shape[self.dim_pos_dic.get("z")] if self.dim_pos_dic.get("z") is not None else 1
                shape_y = array.shape[self.dim_pos_dic.get("y")] if self.dim_pos_dic.get("y") is not None else 1
                shape_x = array.shape[self.dim_pos_dic.get("x")] if self.dim_pos_dic.get("x") is not None else 1
                self.metaData[r, t, c, 'shape'] = (1, 1, shape_z, shape_y, shape_x)

                # change to um if mm
                if self.space_unit == "mm" or self.space_unit == "millimeter":
                    self.metaData[r, t, c, "resolution"] = [val * 1000 for val in self.dataset_scales[r][-3:]]
                else:
                    self.metaData[r, t, c, "resolution"] = self.dataset_scales[r][-3:]

                # Collect dataset info
                self.metaData[r, t, c, "chunks"] = (1, 1, *array.chunks[-3:])
                # dtype = array.dtype
                # if dtype == "int8":
                #     dtype = "uint8"
                # elif dtype == "int16":
                #     dtype = "uint16"
                # elif dtype == "float64" or dtype == "float16":
                #     dtype = "float32"
                self.metaData[r, t, c, "dtype"] = array.dtype
                self.metaData[r, t, c, "ndim"] = array.ndim

                try:
                    self.metaData[r, t, c, "max"] = self.omero["channels"][c]["window"][
                        "end"
                    ]
                    self.metaData[r, t, c, "min"] = self.omero["channels"][c]["window"][
                        "start"
                    ]
                except:
                    pass
            self.arrays[r] = array

            # may not need
            shape_z = array.shape[self.dim_pos_dic["z"]]
            shape_y = array.shape[self.dim_pos_dic["y"]]
            shape_x = array.shape[self.dim_pos_dic["x"]]
            if shape_z <= 64 and shape_y <= 64 and shape_x <= 64:
                self.ResolutionLevels = r + 1
                break

        self.change_resolution_lock(self.ResolutionLevelLock)
        # logger.info(self.metaData)

    def validate_nifti_file(self, file_path):
        """
        Validate the provided NIfTI file.

        Args:
            file_path (str): Path to the NIfTI file.

        Raises:
            Exception: If the file size exceeds the allowed limit.
        """
        if self.file_size > self.allowed_file_size_byte:
                logger.info(f"File '{self.filename}' can not generate pyramid structure. Due to resource constrait, {self.allowed_file_size_gb}GB and below are acceptable for generation process.")
                raise Exception(f"File '{self.filename}' can not generate pyramid structure. Due to resource constrait, {self.allowed_file_size_gb}GB and below are acceptable for generation process.")
        img = nib.load(file_path)
        # try:
            # Attempt to load the file
            # img = nib.load(file_path)
        # except Exception as e:
            # logger.error(f"File '{file_path}' is not a valid nifti file. Error: {e}")
            # raise
            # raise Exception(f"File '{file_path}' is not a valid nifti file. Error: {e}")

    def pyramid_builders(self, nifti_file_location):
        """
        Build a pyramid structure for the NIfTI file.

        Args:
            nifti_file_location (str): Path to the NIfTI file.
        """
        hash_value = calculate_hash(self.file_ino + self.modification_time)
        pyramids_images_store = self.pyramids_images_store
        pyramids_images_store_dir = (
            pyramids_images_store + hash_value[0:2] + "/" + hash_value[2:4] + "/"
        )
        suffix = self.extension_type
        pyramid_image_location = pyramids_images_store_dir + hash_value + suffix
        if self.pyramid_dic.get(hash_value) and os.path.exists(pyramid_image_location):
            self.datapath = self.pyramid_dic.get(hash_value)
            logger.info("Location replaced by generated pyramid image")
        else:
            # Avoid other gunicore workers to build pyramids images
            if os.path.exists(pyramid_image_location):
                logger.info(
                    "Pyramid image was already built by first worker and picked up now by others"
                )
                self.pyramid_dic[hash_value] = pyramid_image_location
                self.datapath = pyramid_image_location
            # 1 hash exists but the pyramid images are deleted during server running
            # 2 no hash and no pyramid images (first time generation)
            else:
                self.pyramid_building_process(
                    nifti_file_location,
                    False,
                    hash_value,
                    pyramids_images_store,
                    pyramids_images_store_dir,
                    pyramid_image_location,
                )

    def pyramid_building_process(
        self,
        nifti_file_location,
        time_axe,
        hash_value,
        pyramids_images_store,
        pyramids_images_store_dir,
        pyramid_image_location,
    ):
        """
        Generate a pyramid structure for a NIfTI file and store it in a specified location.

        This method processes a NIfTI file to create a multi-resolution pyramid structure for 
        efficient image storage and retrieval. It handles multiprocessing, file locking, and 
        storage management to ensure safe and efficient execution.

        Args:
            nifti_file_location (str): The file path to the input NIfTI file.
            time_axe (bool): If True, the time axis is ignored during the conversion process.
            hash_value (str): A unique hash value identifying the image.
            pyramids_images_store (str): The directory where pyramid images are stored.
            pyramids_images_store_dir (str): The specific directory for storing the pyramid image.
            pyramid_image_location (str): The final location of the generated pyramid image.
        """
        os.makedirs(pyramids_images_store_dir, exist_ok=True)
        file_temp = pyramid_image_location.replace(hash_value, "temp_" + hash_value)
        file_temp_lock = file_temp + ".lock"
        file_lock = FileLock(file_temp_lock)
        try:
            with file_lock.acquire():
                logger.success("File lock acquired.")
                if not os.path.exists(pyramid_image_location):
                    logger.success(f"==> Pyramid image is building...")
                    start_time = time.time()
                    # nii2zarr(nifti_file_location,file_temp, no_time=time_axe)

                    # thread = threading.Thread(target=niigz2niizarr,args=(nifti_file_location,file_temp,time_axe))
                    # thread.start()
                    # thread.join()
                    # print("thread complete!")

                    process = multiprocessing.Process(target=separate_process_generation, args=(nifti_file_location, file_temp, time_axe))
                    process.start()
                    process.join()
                    logger.success("Process complete!")

                    end_time= time.time()
                    execution_time = end_time - start_time
                    os.rename(file_temp, pyramid_image_location)
                    logger.success(
                        f"{nifti_file_location} connected to ==> {pyramid_image_location}"
                    )
                    logger.success(
                        f"Pyramid image building complete {nifti_file_location} total execution time: {execution_time}"
                    )
                    if (
                        get_directory_size(pyramids_images_store)
                        > self.allowed_store_size_byte
                    ):
                        delete_oldest_files(
                            pyramids_images_store, self.allowed_store_size_byte
                        )
                else:
                    logger.info("File detected!")
                    if os.path.exists(file_temp):
                        logger.warning('file_temp exist!')
                        os.remove(file_temp)
            self.pyramid_dic[hash_value] = pyramid_image_location
            self.datapath = pyramid_image_location
        except Exception as e:
            logger.error(f"An error occurred during generation process: {e}")
        finally:
            if "data" in locals():
                del data
            gc.collect()
            logger.success("Resources cleaned up.")
            

    def zarr_store_type(self, path):
        """
        Return the appropriate Zarr store for the dataset.

        Args:
            path (str): Path to the dataset.

        Returns:
            zarr.storage: The Zarr store object.
        """
        return self.zarr_store(path)

    def change_resolution_lock(self, ResolutionLevelLock):
        """
        Update the resolution lock and associated metadata.

        Args:
            ResolutionLevelLock (int): The resolution level to lock.
        """
        self.ResolutionLevelLock = ResolutionLevelLock
        # self.shape = self.metaData[self.ResolutionLevelLock, 0, 0, "shape"]
        self.shape = (
            self.TimePoints,
            self.Channels,
            self.metaData[self.ResolutionLevelLock, 0, 0, 'shape'][-3],
            self.metaData[self.ResolutionLevelLock, 0, 0, 'shape'][-2],
            self.metaData[self.ResolutionLevelLock, 0, 0, 'shape'][-1]
        )
        self.ndim = len(self.shape)
        self.chunks = self.metaData[self.ResolutionLevelLock, 0, 0, "chunks"]
        self.resolution = self.metaData[self.ResolutionLevelLock, 0, 0, "resolution"]
        self.dtype = self.metaData[self.ResolutionLevelLock, 0, 0, "dtype"]

    def __getitem__(self, key):
        """
        Access a specific slice of the NIfTI dataset.

        Args:
            key (int, slice, or tuple): Slice specifying the data to access.

        Returns:
            np.ndarray: The requested data slice, optionally squeezed.
        """
        res = 0 if self.ResolutionLevelLock is None else self.ResolutionLevelLock
        logger.info(key)
        if (
            isinstance(key, slice) == False
            and isinstance(key, int) == False
            and len(key) == 6
        ):
            res = key[0]
            if res >= self.ResolutionLevels:
                raise ValueError("Layer is larger than the number of ResolutionLevels")
            key = tuple([x for x in key[1::]])
        logger.info(res)
        logger.info(key)

        if isinstance(key, int):
            key = [slice(key, key + 1)]
            for _ in range(self.ndim - 1):
                key.append(slice(None))
            key = tuple(key)

        if isinstance(key, tuple):
            key = [slice(x, x + 1) if isinstance(x, int) else x for x in key]
            while len(key) < self.ndim:
                key.append(slice(None))
            key = tuple(key)

        logger.info(key)
        newKey = []
        for ss in key:
            if ss.start is None and isinstance(ss.stop, int):
                newKey.append(slice(ss.stop, ss.stop + 1, ss.step))
            else:
                newKey.append(ss)

        key = tuple(newKey)
        logger.info(key)

        array = self.getSlice(r=res, t=key[0], c=key[1], z=key[2], y=key[3], x=key[4])

        if self.squeeze:
            return np.squeeze(array)
        else:
            while len(array.shape) < 5:
                    array = np.expand_dims(array, axis=0)
            return array

    def _get_memorize_cache(
        self, name=None, typed=False, expire=None, tag=None, ignore=()
    ):
        if tag is None:
            tag = self.datapath
        return (
            self.cache.memorize(
                name=name, typed=typed, expire=expire, tag=tag, ignore=ignore
            )
            if self.cache is not None
            else lambda x: x
        )

    def getSlice(self, r, t, c, z, y, x):
        """
        Retrieve a 3D chunk of data for the specified coordinates.

        Args:
            r (int): Resolution level.
            t (slice): Time dimension slice.
            c (slice): Channel dimension slice.
            z (slice): Z-axis slice.
            y (slice): Y-axis slice.
            x (slice): X-axis slice.

        Returns:
            np.ndarray: The requested 3D chunk of data.
        """

        incomingSlices = (r, t, c, z, y, x)
        logger.info(incomingSlices)
        if self.cache is not None:
            # key = f"{self.datapath}_getSlice_{str(incomingSlices)}"
            # key = self.datapath + '_getSlice_' + str(incomingSlices)
            key = f'{self.file_ino + self.modification_time + str(incomingSlices)}'
            result = self.cache.get(key, default=None, retry=True)
            if result is not None:
                logger.info(f"loader cache found")
                return result
        list_tp = [0] * len(self.axes)
        if self.dim_pos_dic.get("t") != None:
            list_tp[self.dim_pos_dic.get("t")] = t
        if self.dim_pos_dic.get("c") != None:
            list_tp[self.dim_pos_dic.get("c")] = c
        if self.dim_pos_dic.get("z") != None:
            list_tp[self.dim_pos_dic.get("z")] = z
        if self.dim_pos_dic.get("y") != None:
            list_tp[self.dim_pos_dic.get("y")] = y
        if self.dim_pos_dic.get("x") != None:
            list_tp[self.dim_pos_dic.get("x")] = x
        tp = tuple(list_tp)
        # logger.success(tp)
        result = self.arrays[r][tp]
        # if len(result.shape) < 4:
        #     result = np.expand_dims(result, axis=0)
        # result = result.astype('uint16')
        logger.info(result.shape)
        if self.cache is not None:
            # print("Cache Status:")
            # shards_limit = self.cache.size_limit / (1024 * 1024 * 1024)  # Convert size_limit to GB
            # shards_len = len(self.cache._shards)  # Number of shards
            # total_size = shards_limit * shards_len  # Total size limit in GB
            # current_size = self.cache.volume() / (1024 * 1024 * 1024)  # Current size in GB

            # print(f"  Shards limit (per shard): {shards_limit} GB")
            # print(f"  Number of shards: {shards_len}")
            # print(f"  Total size limit: {total_size} GB")
            # print(f"  Current size: {current_size} GB\n") 
            self.cache.set(key, result, expire=None, tag=self.file_ino + self.modification_time, retry=True)
            logger.info(f"loader cache saved")
            # test = True
            # while test:
            #     # logger.info('Caching slice')
            #     self.cache.set(key, result, expire=None, tag=self.datapath, retry=True)
            #     if result == self.getSlice(*incomingSlices):
            #         test = False

        return result


    def locationGenerator(self, res):
        """
        Generate the file path for a specific resolution level.

        Args:
            res (int): The resolution level.

        Returns:
            str: The file path corresponding to the resolution level.
        """
        return os.path.join(self.datapath, self.dataset_paths[res])

    def open_array(self, res):
        """
        Open the Zarr array for the specified resolution level.

        Args:
            res (int): The resolution level.

        Returns:
            zarr.core.Array: The Zarr array for the resolution level.
        """
        store = self.zarr_store_type(self.locationGenerator(res))
        logger.info("OPENING ARRAYS")
        #store = self.wrap_store_in_chunk_cache(store)
        # if self.cache is not None:
        #     logger.info('OPENING CHUNK CACHE ARRAYS')
        #     from zarr_stores.zarr_disk_cache import Disk_Cache_Store
        #     store = Disk_Cache_Store(store, unique_id=store.path, diskcache_object=self.cache, persist=False)
        # # try:
        # #     if self.cache is not None:
        # #         store = disk_cache_store(store=store, uuid=self.locationGenerator(res), diskcache_object=self.cache, persist=None, meta_data_expire_min=15)
        # # except Exception as e:
        # #     logger.info('Caught Exception')
        # #     logger.info(e)
        # #     pass
        return zarr.open(store)

    def wrap_store_in_chunk_cache(self, store):
        """
        Wrap the Zarr store with a chunk cache for efficient access.

        Args:
            store (zarr.storage): The Zarr store to wrap.

        Returns:
            zarr.storage: The wrapped Zarr store with chunk caching.
        """
        if self.cache is not None:
            logger.info("OPENING CHUNK CACHE ARRAYS")
            logger.info(store.path)
            from zarr_chunk_cache import disk_cache_store as Disk_Cache_Store

            store = Disk_Cache_Store(
                store, uuid=store.path, diskcache_object=self.cache, persist=True
            )
        return store


# uicontrol bool channel0_visable checkbox(default=true);

# uicontrol invlerp channel0_lut (range=[1.6502681970596313,180.84588623046875],window=[0,180.84588623046875]);

# uicontrol vec3 channel0_color color(default="green");

# vec3 channel0 = vec3(0);


# void main() {

# if (channel0_visable == true)
# channel0 = channel0_color *   channel0_lut();

# vec3 rgb = (channel0);

# vec3 render = min(rgb,vec3(1));

# emitRGB(render);
# }
